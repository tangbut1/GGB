from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
import streamlit as st

from src.collect.news_collector import NewsCollector
from src.preprocess.cleaner import DataCleaner
from src.analysis.sentiment_analysis import SentimentAnalyzer
from src.analysis.trend_prediction import TrendPredictor
from src.visualization.charts import ChartGenerator
from src.visualization.dashboard import DashboardManager
from src.report.export_pdf import PDFReportGenerator
from src.report.export_doc import DOCXReportGenerator
from src.ai_integration import AIClient
from src.data.local_loader import load_local_table


def load_config():
    import yaml
    cfg_path = Path(__file__).parent / "src" / "config.yaml"
    with open(cfg_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def ensure_dirs():
    for p in ["results/charts", "results/logs", "results/reports", "data/processed"]:
        Path(p).mkdir(parents=True, exist_ok=True)


DEFAULT_CATEGORIES = ["ç§‘æŠ€", "é‡‘è", "å›½é™…", "è‚¡ç¥¨"]
DATA_SOURCE_CHOICES: Dict[str, str] = {
    "åœ¨çº¿æ–°é—»é‡‡é›†": "online",
    "æœ¬åœ°è¡¨æ ¼æ•°æ®": "local",
    "åœ¨çº¿ + æœ¬åœ°æ•°æ®": "hybrid"
}
AI_PROVIDER_CHOICES: Dict[str, str] = {
    "è‡ªåŠ¨æ£€æµ‹": "auto",
    "ç¦ç”¨AI": "none",
    "OpenAI": "openai",
    "HuggingFace": "huggingface",
    "è‡ªå®šä¹‰æ¥å£": "custom"
}


def generate_chart_assets(sentiment_data: List[Dict[str, Any]],
                          trend_data: Dict[str, Any]) -> Dict[str, Path]:
    generator = ChartGenerator()
    charts_dir = Path("results/charts")
    charts_dir.mkdir(parents=True, exist_ok=True)

    charts: Dict[str, Path] = {}
    figures = {
        "sentiment_distribution": generator.create_sentiment_distribution_chart(sentiment_data),
        "sentiment_timeline": generator.create_sentiment_timeline_chart(sentiment_data),
        "trend_prediction": generator.create_trend_prediction_chart(trend_data),
        "sentiment_heatmap": generator.create_sentiment_heatmap(sentiment_data)
    }

    for name, fig in figures.items():
        try:
            output_path = charts_dir / f"{name}.png"
            generator.save_chart(fig, str(output_path), format="png")
            charts[name] = output_path
        except Exception as exc:  # noqa: BLE001
            st.warning(f"å›¾è¡¨ {name} ä¿å­˜å¤±è´¥: {exc}")
    return charts


def deduplicate_news(news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    unique: Dict[str, Dict[str, Any]] = {}
    for item in news_list:
        title = str(item.get("title") or item.get("original_title") or "").strip()
        if not title:
            continue
        key = title.lower()
        if key not in unique:
            unique[key] = item
    return list(unique.values())


def build_ai_client(ai_config: Optional[Dict[str, Any]]) -> AIClient:
    ai_config = ai_config or {}
    provider = ai_config.get("provider", "auto")
    if provider == "auto":
        return AIClient.auto_detect()
    if provider == "none":
        return AIClient(provider="none")
    return AIClient(
        provider=provider,
        model=ai_config.get("model"),
        api_key=ai_config.get("api_key"),
        endpoint=ai_config.get("endpoint")
    )


def run_pipeline(data_source: str,
                 selected_categories: List[str],
                 local_records: Optional[List[Dict[str, Any]]] = None,
                 ai_config: Optional[Dict[str, Any]] = None,
                 local_preview: Optional[pd.DataFrame] = None) -> None:
    st.session_state.setdefault("news", [])
    st.session_state.setdefault("cleaned_news", [])
    st.session_state.setdefault("sentiment_results", [])
    st.session_state.setdefault("sentiment_summary", {})
    st.session_state.setdefault("trend_results", {})
    st.session_state.setdefault("trend_summary", {})
    st.session_state.setdefault("chart_paths", {})
    st.session_state.setdefault("ai_summary", {})

    st.write("ğŸš€ MarketPulse: æ•°æ®åˆ†ææµç¨‹å¯åŠ¨...")

    local_records = local_records or []
    aggregated_news: List[Dict[str, Any]] = []
    collector = NewsCollector(categories=selected_categories)

    # 1ï¸âƒ£ æ•°æ®é‡‡é›†
    if data_source in {"online", "hybrid"}:
        with st.spinner("æ­£åœ¨é‡‡é›†æ–°é—»æ•°æ®..."):
            online_news = collector.run_full_pipeline()
            if online_news:
                st.success(f"âœ… å·²é‡‡é›† {len(online_news)} æ¡è´¢ç»æ–°é—»ï¼")
                aggregated_news.extend(online_news)
            else:
                st.warning("âš ï¸ æœªèƒ½è·å–åœ¨çº¿æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–RSSæºã€‚")

    # é¢å¤–åˆå¹¶æœ¬åœ°æ•°æ®
    if data_source in {"local", "hybrid"} and local_records:
        st.success(f"âœ… å·²åŠ è½½ {len(local_records)} æ¡æœ¬åœ°æ•°æ®ã€‚")
        for record in local_records:
            aggregated_news.append({
                "title": record.get("title", ""),
                "content": record.get("content", ""),
                "summary": record.get("summary", ""),
                "publish_time": record.get("publish_time", ""),
                "source": record.get("source", "æœ¬åœ°æ•°æ®"),
                "category": record.get("category", "æœ¬åœ°æ•°æ®"),
                "link": record.get("url") or record.get("link", "")
            })
    elif data_source in {"local", "hybrid"} and not local_records:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°æ•°æ®ï¼Œè¯·å…ˆä¸Šä¼ è¡¨æ ¼æˆ–é€‰æ‹©åœ¨çº¿é‡‡é›†ã€‚")

    aggregated_news = deduplicate_news(aggregated_news)

    if not aggregated_news:
        st.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®ï¼Œç»ˆæ­¢åˆ†ææµç¨‹ã€‚")
        return

    # 2ï¸âƒ£ æ•°æ®æ¸…æ´—
    with st.spinner("æ­£åœ¨æ¸…æ´—æ•°æ®..."):
        cleaner = DataCleaner()
        cleaned_news = cleaner.clean_news_batch(aggregated_news)
        cleaner.save_cleaned_data(cleaned_news)
        st.success(f"âœ… å·²æ¸…æ´— {len(cleaned_news)} æ¡æ–°é—»æ•°æ®ï¼")

    if not cleaned_news:
        st.error("âŒ æ¸…æ´—åæ²¡æœ‰å¯ç”¨çš„æ•°æ®ã€‚")
        return

    # 3ï¸âƒ£ æƒ…ç»ªåˆ†æ
    with st.spinner("æ­£åœ¨è¿›è¡Œæƒ…ç»ªåˆ†æ..."):
        sentiment_analyzer = SentimentAnalyzer()
        analyzed_news = sentiment_analyzer.analyze_news_batch(cleaned_news)
        sentiment_summary = sentiment_analyzer.get_sentiment_summary(analyzed_news)
        sentiment_analyzer.save_analysis_results(analyzed_news, sentiment_summary)
        st.success(f"âœ… æƒ…ç»ªåˆ†æå®Œæˆï¼å¹³å‡æƒ…ç»ªå¾—åˆ†: {sentiment_summary['avg_sentiment']}")

    # 4ï¸âƒ£ è¶‹åŠ¿é¢„æµ‹
    with st.spinner("æ­£åœ¨è¿›è¡Œè¶‹åŠ¿é¢„æµ‹..."):
        trend_predictor = TrendPredictor()
        trend_results = trend_predictor.analyze_market_sentiment_trend(analyzed_news)
        trend_summary = trend_predictor.get_trend_summary(trend_results)
        if 'error' not in trend_results:
            trend_predictor.save_prediction_results(trend_results)
        st.success(f"âœ… è¶‹åŠ¿é¢„æµ‹å®Œæˆï¼è¶‹åŠ¿æ–¹å‘: {trend_summary.get('trend_direction', 'unknown')}")

    # 5ï¸âƒ£ AIå¢å¼ºåˆ†æï¼ˆå¯é€‰ï¼‰
    ai_client = build_ai_client(ai_config)
    ai_scores: List[float] = []
    if ai_client.provider != "none":
        with st.spinner("æ­£åœ¨è¿›è¡ŒAIå¢å¼ºåˆ†æ..."):
            texts = [
                (news.get('original_title') or news.get('title', '')) + " " +
                (news.get('original_content') or news.get('content', ''))
                for news in cleaned_news[:100]
            ]
            try:
                ai_scores = ai_client.classify_sentiment(texts)
                for item, score in zip(analyzed_news, ai_scores):
                    item['ai_sentiment_score'] = score
                st.success(f"âœ… AIåˆ†æå®Œæˆï¼åˆ†æäº† {len(ai_scores)} æ¡æ–‡æœ¬")
            except Exception as exc:  # noqa: BLE001
                st.warning(f"AIåˆ†æå¤±è´¥ï¼š{exc}")

    ai_summary: Dict[str, Any] = {}
    if ai_scores:
        ai_summary = {
            "average": sum(ai_scores) / len(ai_scores),
            "maximum": max(ai_scores),
            "minimum": min(ai_scores),
            "model": ai_client.model or ai_client.provider
        }

    # 6ï¸âƒ£ ç”Ÿæˆå›¾è¡¨èµ„æº
    chart_paths = generate_chart_assets(analyzed_news, trend_results)

    # ä¿å­˜çŠ¶æ€
    st.session_state["news"] = aggregated_news
    st.session_state["cleaned_news"] = cleaned_news
    st.session_state["sentiment_results"] = analyzed_news
    st.session_state["sentiment_summary"] = sentiment_summary
    st.session_state["trend_results"] = trend_results
    st.session_state["trend_summary"] = trend_summary
    st.session_state["chart_paths"] = chart_paths
    st.session_state["ai_summary"] = ai_summary
    st.session_state["data_source"] = data_source
    st.session_state["selected_categories"] = selected_categories
    st.session_state["local_data_preview"] = local_preview.head(20) if isinstance(local_preview, pd.DataFrame) else None
    st.session_state["generated_pdf_path"] = ""
    st.session_state["generated_docx_path"] = ""

    st.success("ğŸ‰ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° results/ æ–‡ä»¶å¤¹")


def render_ai_summary(ai_summary: Optional[Dict[str, Any]]) -> None:
    if not ai_summary:
        return

    st.markdown("---")
    st.subheader("ğŸ§  AIå¢å¼ºåˆ†ææ‘˜è¦")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("å¹³å‡æƒ…ç»ª", f"{ai_summary.get('average', 0):.3f}")
    col2.metric("æœ€é«˜æƒ…ç»ª", f"{ai_summary.get('maximum', 0):.3f}")
    col3.metric("æœ€ä½æƒ…ç»ª", f"{ai_summary.get('minimum', 0):.3f}")
    col4.metric("ä½¿ç”¨æ¨¡å‹", ai_summary.get("model", "-"))


def render_local_preview(preview_df: Optional[pd.DataFrame]) -> None:
    if preview_df is None or preview_df.empty:
        return

    st.markdown("---")
    st.subheader("ğŸ“‚ æœ¬åœ°æ•°æ®é¢„è§ˆï¼ˆå‰20è¡Œï¼‰")
    st.dataframe(preview_df)


def render_report_exports() -> None:
    sentiment_summary = st.session_state.get("sentiment_summary")
    if not sentiment_summary:
        return

    trend_summary = st.session_state.get("trend_summary", {})
    analyzed_news = st.session_state.get("sentiment_results", [])
    trend_results = st.session_state.get("trend_results", {})
    chart_paths = st.session_state.get("chart_paths", {})

    st.markdown("---")
    st.subheader("ğŸ“„ æŠ¥å‘Šå¯¼å‡º")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ“Š ç”ŸæˆPDFæŠ¥å‘Š", key="export_pdf_btn"):
            try:
                pdf_path = PDFReportGenerator().create_report(
                    sentiment_summary,
                    trend_summary,
                    analyzed_news,
                    trend_results,
                    chart_paths=chart_paths
                )
                st.session_state["generated_pdf_path"] = pdf_path
                st.success(f"âœ… PDFæŠ¥å‘Šå·²ç”Ÿæˆ: {pdf_path}")
            except Exception as exc:  # noqa: BLE001
                st.error(f"PDFç”Ÿæˆå¤±è´¥: {exc}")

        pdf_path = st.session_state.get("generated_pdf_path")
        if pdf_path and Path(pdf_path).exists():
            with open(pdf_path, "rb") as pdf_file:
                st.download_button(
                    "ä¸‹è½½PDFæŠ¥å‘Š",
                    data=pdf_file.read(),
                    file_name=Path(pdf_path).name,
                    mime="application/pdf",
                    key="download_pdf_button"
                )

    with col2:
        if st.button("ğŸ“ ç”ŸæˆDOCXæŠ¥å‘Š", key="export_docx_btn"):
            try:
                docx_path = DOCXReportGenerator().create_report(
                    sentiment_summary,
                    trend_summary,
                    analyzed_news,
                    trend_results,
                    chart_paths=chart_paths
                )
                st.session_state["generated_docx_path"] = docx_path
                st.success(f"âœ… DOCXæŠ¥å‘Šå·²ç”Ÿæˆ: {docx_path}")
            except Exception as exc:  # noqa: BLE001
                st.error(f"DOCXç”Ÿæˆå¤±è´¥: {exc}")

        docx_path = st.session_state.get("generated_docx_path")
        if docx_path and Path(docx_path).exists():
            with open(docx_path, "rb") as docx_file:
                st.download_button(
                    "ä¸‹è½½DOCXæŠ¥å‘Š",
                    data=docx_file.read(),
                    file_name=Path(docx_path).name,
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    key="download_docx_button"
                )


def display_results() -> None:
    sentiment_summary = st.session_state.get("sentiment_summary")
    if not sentiment_summary:
        st.info("è¯·å…ˆé…ç½®å‚æ•°å¹¶è¿è¡Œåˆ†ææµç¨‹ã€‚")
        return

    trend_summary = st.session_state.get("trend_summary", {})
    analyzed_news = st.session_state.get("sentiment_results", [])
    trend_results = st.session_state.get("trend_results", {})
    chart_paths = st.session_state.get("chart_paths", {})

    dashboard = DashboardManager()
    dashboard.render_complete_dashboard(
        sentiment_summary,
        trend_summary,
        analyzed_news,
        trend_results,
        chart_paths
    )

    render_ai_summary(st.session_state.get("ai_summary"))
    render_local_preview(st.session_state.get("local_data_preview"))
    render_report_exports()


def main():
    st.set_page_config(page_title="MarketPulse", layout="wide")
    ensure_dirs()
    cfg = load_config()
    st.title("MarketPulse æ™ºèƒ½å¸‚åœºåˆ†æä»ªè¡¨ç›˜")

    state = st.session_state
    state.setdefault("selected_categories", DEFAULT_CATEGORIES)
    state.setdefault("data_source", "online")
    state.setdefault("ai_provider", "auto")
    state.setdefault("ai_model", cfg.get("ai", {}).get("openai_model", ""))
    state.setdefault("ai_endpoint", "")
    state.setdefault("ai_api_key", "")
    state.setdefault("generated_pdf_path", "")
    state.setdefault("generated_docx_path", "")

    local_records: List[Dict[str, Any]] = []
    local_preview_df: Optional[pd.DataFrame] = None

    with st.sidebar:
        st.header("åˆ†æé…ç½®")

        data_source_labels = list(DATA_SOURCE_CHOICES.keys())
        current_source_label = next(
            (label for label, value in DATA_SOURCE_CHOICES.items() if value == state.get("data_source", "online")),
            data_source_labels[0]
        )
        data_source_label = st.selectbox(
            "æ•°æ®æºé€‰æ‹©",
            data_source_labels,
            index=data_source_labels.index(current_source_label)
        )
        data_source = DATA_SOURCE_CHOICES[data_source_label]

        category_options = list(DEFAULT_CATEGORIES)
        selected_categories = st.multiselect(
            "æ–°é—»ç±»åˆ«",
            category_options,
            default=state.get("selected_categories", DEFAULT_CATEGORIES)
        )
        if not selected_categories:
            st.warning("è‡³å°‘é€‰æ‹©ä¸€ä¸ªç±»åˆ«ï¼Œå·²é»˜è®¤é€‰æ‹©å…¨éƒ¨ã€‚")
            selected_categories = DEFAULT_CATEGORIES

        if data_source in {"local", "hybrid"}:
            st.caption("æ”¯æŒCSVã€XLS/XLSXæˆ–JSONæ ¼å¼ï¼Œéœ€åŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰å­—æ®µã€‚")
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ æœ¬åœ°æ•°æ®æ–‡ä»¶",
                type=["csv", "xls", "xlsx", "json"],
                key="local_uploader"
            )
            if uploaded_file is not None:
                local_records, local_preview_df = load_local_table(uploaded_file)
                st.caption(f"å·²è¯»å– {len(local_records)} æ¡æœ¬åœ°æ•°æ®ã€‚")
            else:
                local_records, local_preview_df = [], None

        ai_labels = list(AI_PROVIDER_CHOICES.keys())
        current_ai_label = next(
            (label for label, value in AI_PROVIDER_CHOICES.items() if value == state.get("ai_provider", "auto")),
            ai_labels[0]
        )
        ai_provider_label = st.selectbox(
            "AIæ¨¡å‹æä¾›æ–¹",
            ai_labels,
            index=ai_labels.index(current_ai_label)
        )
        ai_provider = AI_PROVIDER_CHOICES[ai_provider_label]

        ai_model = state.get("ai_model") or cfg.get("ai", {}).get(
            "openai_model" if ai_provider == "openai" else "hf_model", ""
        )
        ai_endpoint = state.get("ai_endpoint", "")
        ai_api_key = state.get("ai_api_key", "")

        if ai_provider in {"openai", "huggingface"}:
            default_model = cfg.get("ai", {}).get(
                "openai_model" if ai_provider == "openai" else "hf_model", ""
            )
            ai_model = st.text_input("æ¨¡å‹åç§°", value=ai_model or default_model, key="ai_model_input")

        if ai_provider == "openai":
            ai_api_key = st.text_input("OpenAI API Key", value=ai_api_key, type="password", key="ai_api_key_input")
            ai_endpoint = st.text_input("API æ¥å£åœ°å€ (å¯é€‰)", value=ai_endpoint, key="ai_endpoint_input")
        elif ai_provider == "huggingface":
            ai_api_key = st.text_input("HuggingFace Token (å¯é€‰)", value=ai_api_key, type="password", key="ai_api_key_input")
            ai_endpoint = st.text_input("æ¨ç†ç«¯ç‚¹ (å¯é€‰)", value=ai_endpoint, key="ai_endpoint_input")
        elif ai_provider == "custom":
            ai_endpoint = st.text_input("è‡ªå®šä¹‰æ¥å£åœ°å€", value=ai_endpoint, key="ai_endpoint_input")
            ai_api_key = st.text_input("æ¥å£å¯†é’¥ (å¯é€‰)", value=ai_api_key, type="password", key="ai_api_key_input")
            ai_model = st.text_input("æ¨¡å‹æ ‡è¯† (å¯é€‰)", value=ai_model, key="ai_model_input_custom")

        st.markdown("---")
        st.caption("æç¤ºï¼šè‹¥é€‰æ‹©è‡ªåŠ¨æ£€æµ‹æˆ–ç¦ç”¨AIï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®æˆ–è·³è¿‡AIåˆ†æã€‚")

    state["data_source"] = data_source
    state["selected_categories"] = selected_categories
    state["ai_provider"] = ai_provider
    state["ai_model"] = ai_model
    state["ai_endpoint"] = ai_endpoint
    state["ai_api_key"] = ai_api_key

    ai_config: Dict[str, Any] = {"provider": ai_provider}
    if ai_provider in {"openai", "huggingface"} and ai_model:
        ai_config["model"] = ai_model
    if ai_provider in {"openai", "huggingface", "custom"} and ai_api_key:
        ai_config["api_key"] = ai_api_key
    if ai_provider in {"openai", "huggingface", "custom"} and ai_endpoint:
        ai_config["endpoint"] = ai_endpoint

    st.markdown("### ğŸ”„ è¿è¡Œåˆ†æ")
    if st.button("è¿è¡Œåˆ†æ", type="primary", use_container_width=True):
        run_pipeline(
            data_source,
            selected_categories,
            local_records,
            ai_config,
            local_preview_df
        )

    display_results()


if __name__ == "__main__":
    main()
