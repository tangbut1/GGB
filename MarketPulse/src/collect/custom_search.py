#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""è‡ªå®šä¹‰å…³é”®è¯æœç´¢é‡‡é›†æ¨¡å—ã€‚

è¯¥ç‰ˆæœ¬ä½¿ç”¨ DuckDuckGo çš„æ–°é—»æ£€ç´¢æ¥å£æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨æœç´¢ï¼Œå¹¶åœ¨å¿…è¦æ—¶
å›é€€åˆ°ä¼ ç»Ÿçš„ç½‘é¡µæŠ“å–ã€‚ä¸ºä¿è¯åˆ†æè´¨é‡ï¼Œæ¨¡å—ä¼šè‡ªåŠ¨è¡¥å……æ–‡ç« æ­£æ–‡ã€æ‘˜è¦ã€
å‘å¸ƒæ—¶é—´ç­‰å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° data/raw ç›®å½•ã€‚
"""

from __future__ import annotations

import json
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from loguru import logger

UserAgent = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
)


class CustomSearchCollector:
    """è‡ªå®šä¹‰æœç´¢æ–°é—»é‡‡é›†å™¨ - æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯æœç´¢ç›¸å…³æ–°é—»"""

    def __init__(self, max_workers: int = 4) -> None:
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)
        self.cutoff_date = datetime.now() - timedelta(days=3)
        self.max_workers = max_workers
        self.min_results = 100

        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": UserAgent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Cache-Control": "no-cache",
            }
        )

    # ------------------------------------------------------------------
    # Public APIs
    # ------------------------------------------------------------------
    def run_custom_search(self, keyword: str, max_results: int = 120) -> List[Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´çš„è‡ªå®šä¹‰æœç´¢æµç¨‹ã€‚"""
        logger.info("ğŸš€ å¼€å§‹è‡ªå®šä¹‰æœç´¢: %s", keyword)

        news_list = self.search_news(keyword, max_results=max_results)
        if not news_list:
            logger.error("âŒ æœªæ‰¾åˆ°ä¸ %s ç›¸å…³çš„å®æ—¶æ–°é—»ï¼Œè¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚", keyword)
            return []

        self.save_news(news_list, keyword)
        logger.success("ğŸ‰ è‡ªå®šä¹‰æœç´¢å®Œæˆï¼å…±è·å– %s æ¡æ–°é—»", len(news_list))
        if len(news_list) < self.min_results:
            logger.warning(
                "âš ï¸ æœç´¢ç»“æœä»… %s æ¡ï¼Œæœªè¾¾åˆ° %s æ¡ç›®æ ‡ï¼Œå»ºè®®å°è¯•æ›´å…·ä½“æˆ–æ›´å¹¿æ³›çš„å…³é”®è¯ç»„åˆã€‚",
                len(news_list),
                self.min_results,
            )
        return news_list

    def search_news(self, keyword: str, max_results: int = 120) -> List[Dict[str, Any]]:
        """æ ¹æ®å…³é”®è¯æœç´¢æ–°é—»å¹¶åšé¢„å¤„ç†ã€‚"""
        logger.info("ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: %s", keyword)
        aggregated: List[Dict[str, Any]] = []
        target_results = max(max_results, self.min_results)

        primary_terms = [keyword]
        auxiliary_terms = [
            f"{keyword} æœ€æ–°",
            f"{keyword} æ–°é—»",
            f"{keyword} å¸‚åœº",
            f"{keyword} è¶‹åŠ¿",
        ]

        for term in primary_terms + [t for t in auxiliary_terms if t not in primary_terms]:
            ddg_results = self._search_duckduckgo(term, max_results=target_results)
            aggregated.extend(ddg_results)
            if len(aggregated) >= target_results:
                break

        if len(aggregated) < target_results:
            logger.info("DuckDuckGo ç»“æœä¸è¶³ï¼Œå°è¯•è¡¥å……é€šç”¨ç½‘é¡µæŠ“å–...")
            aggregated.extend(self._search_generic(keyword, remaining=target_results - len(aggregated)))

        if len(aggregated) < target_results:
            for term in auxiliary_terms:
                if len(aggregated) >= target_results:
                    break
                aggregated.extend(self._search_generic(term, remaining=target_results - len(aggregated)))

        if not aggregated:
            return []

        deduplicated = self._deduplicate_news(aggregated)
        self._enrich_news_content(deduplicated)

        for item in deduplicated:
            publish_dt = item.pop("_publish_dt", None)
            if isinstance(publish_dt, datetime):
                item["publish_time"] = publish_dt.strftime("%Y-%m-%d %H:%M")

        deduplicated.sort(key=lambda x: x.get("publish_time", ""), reverse=True)
        logger.success("ğŸ‰ æœç´¢å®Œæˆï¼å…±è·å– %s æ¡ç›¸å…³æ–°é—»", len(deduplicated))
        return deduplicated[:target_results]

    def save_news(self, news_list: List[Dict[str, Any]], keyword: str) -> None:
        """ä¿å­˜æœç´¢åˆ°çš„æ–°é—»æ•°æ®åˆ°æœ¬åœ° JSON æ–‡ä»¶ã€‚"""
        if not news_list:
            logger.warning("æ²¡æœ‰æ–°é—»æ•°æ®éœ€è¦ä¿å­˜")
            return

        safe_keyword = re.sub(r"[^\w\s-]", "", keyword).strip()
        safe_keyword = re.sub(r"[-\s]+", "_", safe_keyword)
        filename = f"custom_search_{safe_keyword}_{int(time.time())}.json"

        raw_file_path = self.data_dir / "raw" / filename
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)

        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)

        logger.success("âœ… å·²ä¿å­˜ %s æ¡æ–°é—»åˆ° %s", len(news_list), raw_file_path)

    # ------------------------------------------------------------------
    # DuckDuckGo search
    # ------------------------------------------------------------------
    def _search_duckduckgo(self, keyword: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ DuckDuckGo æ–°é—»æœç´¢æ¥å£ã€‚"""
        results: List[Dict[str, Any]] = []
        try:
            logger.info("ä½¿ç”¨ DuckDuckGo æœç´¢ %s", keyword)
            with DDGS(timeout=20) as ddgs:
                for item in ddgs.news(
                    keywords=keyword,
                    region="cn-zh",
                    safesearch="moderate",
                    max_results=max_results,
                ):
                    title = (item.get("title") or "").strip()
                    url = (item.get("url") or item.get("link") or "").strip()
                    if not title or not url:
                        continue

                    publish_dt = self._parse_datetime(item.get("date"))
                    if publish_dt and publish_dt < self.cutoff_date:
                        continue

                    summary = (item.get("body") or "").strip()
                    source = (item.get("source") or "DuckDuckGo").strip()

                    results.append(
                        {
                            "title": title,
                            "original_title": title,
                            "link": url,
                            "summary": summary,
                            "source": source or "DuckDuckGo",
                            "publish_time": item.get("date", ""),
                            "category": "è‡ªå®šä¹‰æœç´¢",
                            "search_keyword": keyword,
                            "_publish_dt": publish_dt or datetime.now(),
                        }
                    )
        except Exception as exc:  # noqa: BLE001
            logger.error("âŒ DuckDuckGo æœç´¢å¤±è´¥: %s", exc)
        return results

    # ------------------------------------------------------------------
    # Generic fallback
    # ------------------------------------------------------------------
    def _search_generic(self, keyword: str, remaining: int) -> List[Dict[str, Any]]:
        """ç®€å•çš„ç½‘é¡µæŠ“å–å›é€€æ–¹æ¡ˆï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨æœç´¢ç»“æœé¡µã€‚"""
        if remaining <= 0:
            return []

        logger.info("å°è¯•é€šè¿‡ HTML æŠ“å–è¡¥å……æœç´¢ç»“æœ...")
        url = "https://www.bing.com/news/search"
        params = {"q": keyword, "mkt": "zh-CN", "count": str(min(remaining, 30))}
        items: List[Dict[str, Any]] = []

        try:
            resp = self.session.get(url, params=params, timeout=15)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            for element in soup.select("div.news-card"):
                title_elem = element.select_one("a.title") or element.select_one("a")
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                link = title_elem.get("href", "")
                if not title or not link:
                    continue

                summary_elem = element.select_one("div.snippet") or element.select_one("div.snippet span")
                summary = summary_elem.get_text(strip=True) if summary_elem else ""

                source_elem = element.select_one("div.source") or element.select_one("div.source span")
                source = source_elem.get_text(strip=True) if source_elem else "Bing æ–°é—»"

                time_elem = element.select_one("span.time")
                publish_time = time_elem.get_text(strip=True) if time_elem else ""

                publish_dt = self._parse_datetime(publish_time) or datetime.now()
                if publish_dt < self.cutoff_date:
                    continue

                items.append(
                    {
                        "title": title,
                        "original_title": title,
                        "link": link,
                        "summary": summary,
                        "source": source,
                        "publish_time": publish_time,
                        "category": "è‡ªå®šä¹‰æœç´¢",
                        "search_keyword": keyword,
                        "_publish_dt": publish_dt,
                    }
                )
        except Exception as exc:  # noqa: BLE001
            logger.error("é€šç”¨ç½‘é¡µæŠ“å–å¤±è´¥: %s", exc)

        return items

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _enrich_news_content(self, news_items: List[Dict[str, Any]]) -> None:
        """ä¸ºæœç´¢ç»“æœè¡¥å……æ­£æ–‡å’Œæ‘˜è¦ä¿¡æ¯ã€‚"""
        if not news_items:
            return

        tasks = [item for item in news_items if item.get("link")]
        if not tasks:
            return

        max_items = min(len(tasks), 12)
        selected = tasks[:max_items]

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_map = {
                executor.submit(self._extract_article_text, item["link"]): item for item in selected
            }

            for future in as_completed(future_map):
                item = future_map[future]
                try:
                    content, summary = future.result()
                except Exception as exc:  # noqa: BLE001
                    logger.debug("æå– %s å†…å®¹å¤±è´¥: %s", item.get("link"), exc)
                    continue

                if content:
                    item["content"] = content
                if summary and not item.get("summary"):
                    item["summary"] = summary

    def _extract_article_text(self, url: str) -> Tuple[str, str]:
        """ä»æ–‡ç« é¡µé¢ä¸­æå–æ­£æ–‡ä¸æ‘˜è¦ã€‚"""
        resp = self.session.get(url, timeout=15)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")

        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        paragraphs = [p for p in paragraphs if p]
        if not paragraphs:
            return "", ""

        text = "\n".join(paragraphs)
        text = re.sub(r"\s+", " ", text).strip()

        # ç”Ÿæˆæ‘˜è¦
        summary = "".join(paragraphs[:3])
        summary = summary[:280] + ("..." if len(summary) > 280 else "")
        content = text[:5000]
        return content, summary

    def _parse_datetime(self, value: Optional[str]) -> Optional[datetime]:
        """è§£æå¤šç§æ¥æºçš„æ—¶é—´å­—ç¬¦ä¸²ã€‚"""
        if not value:
            return None

        if isinstance(value, datetime):
            return value if value.tzinfo is None else value.replace(tzinfo=None)

        value = str(value).strip()
        if not value:
            return None

        # DuckDuckGo è¿”å› ISO å­—ç¬¦ä¸²ï¼Œå¤„ç† Z ç»“å°¾
        if value.endswith("Z"):
            value = value[:-1] + "+00:00"

        iso_formats = (
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%Y-%m-%d",
            "%Y/%m/%d %H:%M",
            "%Y.%m.%d %H:%M",
        )

        try:
            dt = datetime.fromisoformat(value)
            return dt.replace(tzinfo=None)
        except Exception:  # noqa: BLE001
            pass

        for fmt in iso_formats:
            try:
                return datetime.strptime(value, fmt)
            except Exception:  # noqa: BLE001
                continue

        # å¤„ç†ç±»ä¼¼ â€œ3å°æ—¶å‰â€ çš„ç›¸å¯¹æ—¶é—´
        relative = re.match(r"(\d+)(åˆ†é’Ÿ|å°æ—¶|å¤©)å‰", value)
        if relative:
            amount = int(relative.group(1))
            unit = relative.group(2)
            delta = {
                "åˆ†é’Ÿ": timedelta(minutes=amount),
                "å°æ—¶": timedelta(hours=amount),
                "å¤©": timedelta(days=amount),
            }.get(unit, timedelta())
            return datetime.now() - delta

        return None

    def _deduplicate_news(self, news_list: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰é“¾æ¥ä¼˜å…ˆå»é‡ï¼Œä¿ç•™ä¸åŒæ¥æºçš„é‡å¤æ ‡é¢˜ã€‚"""
        unique: Dict[str, Dict[str, Any]] = {}
        for news in news_list:
            title = (news.get("title") or "").strip().lower()
            link = (news.get("link") or news.get("url") or "").strip().lower()
            source = (news.get("source") or "").strip().lower()
            if not title and not link:
                continue
            key = link if link else f"{title}::{source}"
            if key not in unique:
                unique[key] = news
        return list(unique.values())

    def _create_sample_news(self, keyword: str) -> List[Dict[str, Any]]:
        """åˆ›å»ºç¤ºä¾‹æ–°é—»æ•°æ®ã€‚"""
        today = datetime.now().strftime("%Y-%m-%d")
        sample_news = [
            {
                "title": f"{keyword} ç›¸å…³æ–°é—»ï¼šå¸‚åœºåŠ¨æ€åˆ†æ",
                "link": "https://example.com/news1",
                "summary": f"å…³äº {keyword} çš„æœ€æ–°å¸‚åœºåŠ¨æ€å’Œå‘å±•è¶‹åŠ¿åˆ†æã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": today,
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword,
                "content": f"è¿™æ˜¯ä¸€æ¡å…³äº {keyword} çš„ç¤ºä¾‹æ–°é—»ï¼Œç”¨äºåœ¨çœŸå®æ•°æ®ç¼ºå¤±æ—¶å±•ç¤ºåº”ç”¨æµç¨‹ã€‚",
            },
            {
                "title": f"{keyword} è¡Œä¸šæŠ¥å‘Šï¼šæŠ•èµ„æœºä¼šåˆ†æ",
                "link": "https://example.com/news2",
                "summary": f"æ·±å…¥åˆ†æ {keyword} è¡Œä¸šçš„æŠ•èµ„æœºä¼šå’Œé£é™©å› ç´ ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": today,
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword,
                "content": f"ç¤ºä¾‹æ–°é—»å±•ç¤º {keyword} åœ¨è¡Œä¸šä¸­çš„æŠ•èµ„äº®ç‚¹ä¸æ½œåœ¨é£é™©ã€‚",
            },
            {
                "title": f"{keyword} æŠ€æœ¯å‘å±•ï¼šåˆ›æ–°çªç ´",
                "link": "https://example.com/news3",
                "summary": f"{keyword} é¢†åŸŸçš„æŠ€æœ¯åˆ›æ–°å’Œçªç ´æ€§è¿›å±•ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": today,
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword,
                "content": f"è¯¥ç¤ºä¾‹æ–°é—»æè¿°äº† {keyword} ç›¸å…³çš„æ–°æŠ€æœ¯ä¸è¡Œä¸šè¶‹åŠ¿ã€‚",
            },
        ]

        logger.info("ğŸ“ å·²åˆ›å»º %s æ¡ç¤ºä¾‹æ•°æ®", len(sample_news))
        return sample_news


__all__ = ["CustomSearchCollector"]
