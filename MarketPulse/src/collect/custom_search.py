#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import time
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime, timedelta
from loguru import logger
from bs4 import BeautifulSoup
import re


class CustomSearchCollector:
    """è‡ªå®šä¹‰æœç´¢æ–°é—»é‡‡é›†å™¨ - æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯æœç´¢ç›¸å…³æ–°é—»"""
    
    def __init__(self):
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¶é—´è¿‡æ»¤ï¼šåªæŠ“å–3å¤©å†…çš„æ•°æ®ï¼ˆè‡ªå®šä¹‰æœç´¢ï¼‰
        self.cutoff_date = datetime.now() - timedelta(days=3)
        
        # æœç´¢æºé…ç½®
        self.search_sources = {
            "google": {
                "base_url": "https://www.google.com/search",
                "params": {
                    "q": "",  # æœç´¢å…³é”®è¯
                    "tbm": "nws",  # æ–°é—»æœç´¢
                    "num": "20",   # ç»“æœæ•°é‡
                    "hl": "zh-CN", # ä¸­æ–‡
                    "gl": "CN"     # ä¸­å›½
                },
                "headers": {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            },
            "bing": {
                "base_url": "https://www.bing.com/news/search",
                "params": {
                    "q": "",  # æœç´¢å…³é”®è¯
                    "count": "20",
                    "mkt": "zh-CN"
                },
                "headers": {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            }
        }
    
    def search_news(self, keyword: str, max_results: int = 80) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢æ–°é—»
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        logger.info(f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        all_news = []
        
        # ä»å¤šä¸ªæºæœç´¢ï¼Œå¢åŠ æ¯ä¸ªæºçš„æœç´¢æ•°é‡
        per_source_results = max_results // len(self.search_sources)
        for source_name, config in self.search_sources.items():
            try:
                logger.info(f"æ­£åœ¨ä» {source_name} æœç´¢...")
                news = self._search_from_source(keyword, source_name, config, per_source_results)
                all_news.extend(news)
                logger.success(f"âœ… ä» {source_name} è·å– {len(news)} æ¡æ–°é—»")
            except Exception as e:
                logger.error(f"âŒ ä» {source_name} æœç´¢å¤±è´¥: {e}")
                continue
        
        # å»é‡
        unique_news = self._deduplicate_news(all_news)
        
        logger.success(f"ğŸ‰ æœç´¢å®Œæˆï¼å…±è·å– {len(unique_news)} æ¡ç›¸å…³æ–°é—»")
        return unique_news
    
    def _search_from_source(self, keyword: str, source_name: str, config: Dict[str, Any], max_results: int) -> List[Dict[str, Any]]:
        """ä»æŒ‡å®šæºæœç´¢æ–°é—»"""
        if source_name == "google":
            return self._search_google(keyword, config, max_results)
        elif source_name == "bing":
            return self._search_bing(keyword, config, max_results)
        else:
            return []
    
    def _search_google(self, keyword: str, config: Dict[str, Any], max_results: int) -> List[Dict[str, Any]]:
        """ä»Googleæœç´¢æ–°é—»"""
        try:
            # ä½¿ç”¨æ›´çœŸå®çš„æœç´¢å‚æ•°
            params = {
                "q": f"{keyword} news",
                "tbm": "nws",  # æ–°é—»æœç´¢
                "num": str(max_results),
                "hl": "zh-CN",
                "gl": "CN",
                "tbs": "qdr:d"  # æœ€è¿‘ä¸€å¤©
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(
                "https://www.google.com/search", 
                params=params, 
                headers=headers,
                timeout=15
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = []
            
            # è§£æGoogleæ–°é—»ç»“æœ - ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
            news_elements = soup.find_all('div', class_='g') or soup.find_all('div', class_='WlydOe')
            
            for element in news_elements[:max_results]:
                try:
                    # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
                    title_elem = (element.find('h3') or 
                                element.find('div', class_='n0jPhd') or
                                element.find('a', class_='WlydOe'))
                    if not title_elem:
                        continue
                    title = title_elem.get_text().strip()
                    
                    # æå–é“¾æ¥
                    link_elem = element.find('a')
                    if not link_elem:
                        continue
                    link = link_elem.get('href', '')
                    if link.startswith('/url?q='):
                        link = link.split('/url?q=')[1].split('&')[0]
                    
                    # æå–æ‘˜è¦
                    summary_elem = (element.find('div', class_='VwiC3b') or
                                  element.find('div', class_='GI74Re') or
                                  element.find('span', class_='st'))
                    summary = summary_elem.get_text().strip() if summary_elem else ""
                    
                    # æå–æ¥æºå’Œæ—¶é—´
                    source_elem = (element.find('span', class_='CEMjEf') or
                                 element.find('span', class_='WF4CUc'))
                    source = source_elem.get_text().strip() if source_elem else "Googleæœç´¢"
                    
                    time_elem = (element.find('span', class_='LEwnzc') or
                               element.find('span', class_='f'))
                    publish_time = time_elem.get_text().strip() if time_elem else ""
                    
                    if title and link and len(title) > 5:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                        # æ£€æŸ¥æ—¶é—´æ˜¯å¦åœ¨3å¤©å†…
                        if self._is_recent_news(publish_time):
                            news_items.append({
                                "title": title,
                                "link": link,
                                "summary": summary,
                                "source": source,
                                "publish_time": publish_time,
                                "category": "è‡ªå®šä¹‰æœç´¢",
                                "search_keyword": keyword
                            })
                except Exception as e:
                    logger.warning(f"è§£æGoogleæ–°é—»é¡¹å¤±è´¥: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"Googleæœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_bing(self, keyword: str, config: Dict[str, Any], max_results: int) -> List[Dict[str, Any]]:
        """ä»Bingæœç´¢æ–°é—»"""
        try:
            params = config["params"].copy()
            params["q"] = keyword
            
            response = requests.get(
                config["base_url"], 
                params=params, 
                headers=config["headers"],
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = []
            
            # è§£æBingæ–°é—»ç»“æœ
            news_elements = soup.find_all('div', class_='news-card')
            
            for element in news_elements[:max_results]:
                try:
                    # æå–æ ‡é¢˜
                    title_elem = element.find('h2')
                    if not title_elem:
                        continue
                    title = title_elem.get_text().strip()
                    
                    # æå–é“¾æ¥
                    link_elem = element.find('a')
                    if not link_elem:
                        continue
                    link = link_elem.get('href', '')
                    
                    # æå–æ‘˜è¦
                    summary_elem = element.find('p')
                    summary = summary_elem.get_text().strip() if summary_elem else ""
                    
                    # æå–æ¥æºå’Œæ—¶é—´
                    source_elem = element.find('span', class_='source')
                    source = source_elem.get_text().strip() if source_elem else "Bingæœç´¢"
                    
                    time_elem = element.find('span', class_='time')
                    publish_time = time_elem.get_text().strip() if time_elem else ""
                    
                    if title and link:
                        news_items.append({
                            "title": title,
                            "link": link,
                            "summary": summary,
                            "source": source,
                            "publish_time": publish_time,
                            "category": "è‡ªå®šä¹‰æœç´¢",
                            "search_keyword": keyword
                        })
                except Exception as e:
                    logger.warning(f"è§£æBingæ–°é—»é¡¹å¤±è´¥: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"Bingæœç´¢å¤±è´¥: {e}")
            return []
    
    def _deduplicate_news(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ–°é—»"""
        unique_news = {}
        for news in news_list:
            title = news.get("title", "").strip().lower()
            if title and title not in unique_news:
                unique_news[title] = news
        return list(unique_news.values())
    
    def _is_recent_news(self, published_str: str) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨3å¤©å†…å‘å¸ƒ"""
        if not published_str:
            return True  # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«
        
        try:
            # å°è¯•è§£æå„ç§æ—¶é—´æ ¼å¼
            published_time = None
            
            # å¸¸è§çš„æ—¶é—´æ ¼å¼
            time_formats = [
                "%a, %d %b %Y %H:%M:%S %Z",  # RFC 2822
                "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822 with timezone
                "%Y-%m-%d %H:%M:%S",          # ISO format
                "%Y-%m-%dT%H:%M:%S",         # ISO format with T
                "%Y-%m-%dT%H:%M:%SZ",       # ISO format with Z
                "%Y-%m-%dT%H:%M:%S.%fZ",    # ISO format with microseconds
                "%a, %d %b %Y %H:%M:%S",    # Without timezone
            ]
            
            for fmt in time_formats:
                try:
                    published_time = datetime.strptime(published_str, fmt)
                    break
                except ValueError:
                    continue
            
            if published_time is None:
                # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨email.utilsçš„æ—¶é—´è§£æ
                try:
                    import email.utils
                    published_time = email.utils.parsedate_to_datetime(published_str)
                except:
                    return True  # è§£æå¤±è´¥æ—¶é»˜è®¤åŒ…å«
            
            # æ£€æŸ¥æ˜¯å¦åœ¨3å¤©å†…
            return published_time >= self.cutoff_date
            
        except Exception as e:
            logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {published_str} - {e}")
            return True  # è§£æå¤±è´¥æ—¶é»˜è®¤åŒ…å«
    
    def save_news(self, news_list: List[Dict[str, Any]], keyword: str) -> None:
        """ä¿å­˜æœç´¢åˆ°çš„æ–°é—»"""
        if not news_list:
            logger.warning("æ²¡æœ‰æ–°é—»æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        # åˆ›å»ºæ–‡ä»¶å
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).strip()
        safe_keyword = re.sub(r'[-\s]+', '_', safe_keyword)
        filename = f"custom_search_{safe_keyword}_{int(time.time())}.json"
        
        # ä¿å­˜è·¯å¾„
        raw_file_path = self.data_dir / "raw" / filename
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å·²ä¿å­˜ {len(news_list)} æ¡æ–°é—»åˆ° {raw_file_path}")
    
    def run_custom_search(self, keyword: str, max_results: int = 80) -> List[Dict[str, Any]]:
        """è¿è¡Œè‡ªå®šä¹‰æœç´¢æµç¨‹"""
        logger.info(f"ğŸš€ å¼€å§‹è‡ªå®šä¹‰æœç´¢: {keyword}")
        
        # 1. æœç´¢æ–°é—»
        news_list = self.search_news(keyword, max_results)
        
        if not news_list:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®")
            news_list = self._create_sample_news(keyword)
        
        # 2. ä¿å­˜æ•°æ®
        self.save_news(news_list, keyword)
        
        logger.success(f"ğŸ‰ è‡ªå®šä¹‰æœç´¢å®Œæˆï¼å…±è·å– {len(news_list)} æ¡æ–°é—»")
        return news_list
    
    def _create_sample_news(self, keyword: str) -> List[Dict[str, Any]]:
        """åˆ›å»ºç¤ºä¾‹æ–°é—»æ•°æ®"""
        sample_news = [
            {
                "title": f"{keyword}ç›¸å…³æ–°é—»ï¼šå¸‚åœºåŠ¨æ€åˆ†æ",
                "link": f"https://example.com/news1",
                "summary": f"å…³äº{keyword}çš„æœ€æ–°å¸‚åœºåŠ¨æ€å’Œå‘å±•è¶‹åŠ¿åˆ†æã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": "2024-10-24",
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword
            },
            {
                "title": f"{keyword}è¡Œä¸šæŠ¥å‘Šï¼šæŠ•èµ„æœºä¼šåˆ†æ",
                "link": f"https://example.com/news2",
                "summary": f"æ·±å…¥åˆ†æ{keyword}è¡Œä¸šçš„æŠ•èµ„æœºä¼šå’Œé£é™©å› ç´ ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": "2024-10-24",
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword
            },
            {
                "title": f"{keyword}æŠ€æœ¯å‘å±•ï¼šåˆ›æ–°çªç ´",
                "link": f"https://example.com/news3",
                "summary": f"{keyword}é¢†åŸŸçš„æŠ€æœ¯åˆ›æ–°å’Œçªç ´æ€§è¿›å±•ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "publish_time": "2024-10-24",
                "category": "è‡ªå®šä¹‰æœç´¢",
                "search_keyword": keyword
            }
        ]
        
        logger.info(f"ğŸ“ å·²åˆ›å»º {len(sample_news)} æ¡ç¤ºä¾‹æ•°æ®")
        return sample_news
