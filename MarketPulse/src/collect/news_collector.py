import os
import json
import hashlib
import feedparser
import requests
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Tuple
from loguru import logger


class NewsCollector:
    """å¤šæºè´¢ç»æ–°é—»é‡‡é›†å™¨ - æ”¯æŒå¤šä¸ªRSSæºï¼Œè‡ªåŠ¨å»é‡å’Œå®¹é”™"""

    def __init__(self, categories=None):
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ—¶é—´è¿‡æ»¤ï¼šåªæŠ“å–æœ€è¿‘3å¤©å†…çš„æ•°æ®
        self.cutoff_date = datetime.now() - timedelta(days=3)
        self.min_results = 100

        # å¤šæºRSSé…ç½®ï¼ˆæŒ‰ç±»åˆ«ç»„ç»‡ï¼Œä¾¿äºç”¨æˆ·é€‰æ‹©ï¼‰

        self.category_feeds = {
            "ç§‘æŠ€": [
                "https://feeds.bbci.co.uk/news/technology/rss.xml",
                "https://feeds.reuters.com/reuters/technologyNews",
                "https://feeds.feedburner.com/oreilly/radar",
                "https://www.theverge.com/rss/index.xml",
                "https://feeds.arstechnica.com/arstechnica/index",
                "https://www.wired.com/feed/rss"
            ],
            "é‡‘è": [
                "https://feeds.bbci.co.uk/news/business/rss.xml",
                "https://feeds.reuters.com/reuters/businessNews",
                "https://feeds.marketwatch.com/marketwatch/topstories/",
                "https://www.cnbc.com/id/10000664/device/rss/rss.html",
                "https://www.ft.com/rss/home/asia",
                "https://www.economist.com/finance-and-economics/rss.xml"
            ],
            "å›½é™…": [
                "https://feeds.bbci.co.uk/news/world/rss.xml",
                "https://feeds.reuters.com/reuters/worldNews",
                "https://feeds.a.dj.com/rss/RSSWorldNews.xml",
                "https://feeds.cnn.com/rss/edition.rss",
                "https://feeds.npr.org/1001/rss.xml",
                "https://feeds.nytimes.com/nyt/World.xml",
                "https://www.aljazeera.com/xml/rss/all.xml",
                "https://www.scmp.com/rss/91/feed"
            ],
            "è‚¡ç¥¨": [
                "https://feeds.marketwatch.com/marketwatch/marketpulse/",
                "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
                "https://feeds.finance.yahoo.com/rss/2.0/headline",
                "https://seekingalpha.com/market_currents.xml",
                "https://www.nasdaq.com/feed/rssoutbound?category=MarketHeadlines",
                "https://finance.yahoo.com/news/rssindex"
            ]
        }

        self.category_alias_map = {
            "tech": "ç§‘æŠ€",
            "ç§‘æŠ€": "ç§‘æŠ€",
            "technology": "ç§‘æŠ€",
            "finance": "é‡‘è",
            "é‡‘è": "é‡‘è",
            "international": "å›½é™…",
            "å›½é™…": "å›½é™…",
            "global": "å›½é™…",
            "stock": "è‚¡ç¥¨",
            "stocks": "è‚¡ç¥¨",
            "è‚¡ç¥¨": "è‚¡ç¥¨"
        }

        self.selected_categories = self._normalize_categories(categories)

    def set_categories(self, categories=None):
        """æ›´æ–°ç”¨æˆ·é€‰æ‹©çš„ç±»åˆ«"""
        self.selected_categories = self._normalize_categories(categories)

    def _normalize_categories(self, categories=None):
        if not categories:
            return list(self.category_feeds.keys())

        normalized = []
        for category in categories:
            if not category:
                continue
            key = self.category_alias_map.get(str(category).strip().lower(), category)
            if key in self.category_feeds and key not in normalized:
                normalized.append(key)
        return normalized or list(self.category_feeds.keys())
    
    def _is_recent_news(self, published_str: str) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨3å¤©å†…å‘å¸ƒ"""
        if not published_str:
            return True  # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«

        try:
            published_time = None
            time_formats = [
                "%a, %d %b %Y %H:%M:%S %Z",
                "%a, %d %b %Y %H:%M:%S %z",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%dT%H:%M:%SZ",
                "%Y-%m-%dT%H:%M:%S.%fZ",
                "%a, %d %b %Y %H:%M:%S",
            ]

            for fmt in time_formats:
                try:
                    published_time = datetime.strptime(published_str, fmt)
                    break
                except ValueError:
                    continue

            if published_time is None:
                try:
                    import email.utils
                    published_time = email.utils.parsedate_to_datetime(published_str)
                except Exception:
                    return True

            return published_time >= self.cutoff_date
        except Exception as e:
            logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {published_str} - {e}")
            return True

    def _collect_feed_entries(self, category: str, url: str) -> List[Dict[str, Any]]:
        """é‡‡é›†å•ä¸ªRSSæºçš„æ–°é—»åˆ—è¡¨"""
        logger.info(f"Fetching {category} news from {url} ...")
        try:
            feed = feedparser.parse(url)
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"RSSè§£æè­¦å‘Š: {url} - {getattr(feed, 'bozo_exception', 'Unknown error')}")
            if not feed.entries:
                logger.warning(f"No entries found in {url}")
                return []

            source_news: List[Dict[str, Any]] = []
            for entry in feed.entries:
                if not self._is_recent_news(entry.get("published", "")):
                    continue
                item = {
                    "title": entry.get("title", "").strip(),
                    "link": entry.get("link", "").strip(),
                    "published": entry.get("published", ""),
                    "summary": entry.get("summary", "").strip(),
                    "content": entry.get("content", [{}])[0].get("value", "") if entry.get("content") else "",
                    "source": url,
                    "category": category
                }
                if item["title"] and item["link"]:
                    source_news.append(item)

            if source_news:
                logger.success(f"âœ… æˆåŠŸä» {url} è·å– {len(source_news)} æ¡ {category} ç±»æ–°é—»")
            else:
                logger.warning(f"ä» {url} è·å–çš„æ–°é—»ä¸ºç©ºæˆ–æ— æ•ˆ")
            return source_news
        except Exception as e:
            logger.error(f"âŒ ä» {url} æŠ“å–å¤±è´¥: {e}")
            return []

    def fetch_latest(self) -> List[Dict[str, Any]]:
        """ä»å¤šä¸ªRSSæºæŠ“å–æ–°é—»ï¼Œç›®æ ‡ä¸å°‘äº100æ¡"""
        all_news: List[Dict[str, Any]] = []
        successful_sources = 0
        processed_urls = set()

        categories = self.selected_categories or list(self.category_feeds.keys())
        planned_feeds: List[Tuple[str, str]] = []
        for category in categories:
            feeds = self.category_feeds.get(category, [])
            planned_feeds.extend([(category, url) for url in feeds])

        if not planned_feeds:
            logger.warning("æœªæ‰¾åˆ°åŒ¹é…çš„RSSæºï¼Œä½¿ç”¨æ‰€æœ‰é»˜è®¤æºã€‚")
            for category, urls in self.category_feeds.items():
                planned_feeds.extend([(category, url) for url in urls])
            categories = list(self.category_feeds.keys())

        for category, url in planned_feeds:
            if url in processed_urls:
                continue
            processed_urls.add(url)
            entries = self._collect_feed_entries(category, url)
            if entries:
                all_news.extend(entries)
                successful_sources += 1
            if len(all_news) >= self.min_results:
                break

        if len(all_news) < self.min_results:
            remaining_categories = [cat for cat in self.category_feeds if cat not in categories]
            if remaining_categories:
                logger.warning(
                    f"å½“å‰æ‰€é€‰ç±»åˆ«ä»…è·å–åˆ° {len(all_news)} æ¡æ–°é—»ï¼Œè‡ªåŠ¨è¡¥å……å…¶ä»–ç±»åˆ«ä»¥è¾¾åˆ° {self.min_results} æ¡ç›®æ ‡ã€‚"
                )
                for category in remaining_categories:
                    for url in self.category_feeds.get(category, []):
                        if url in processed_urls:
                            continue
                        processed_urls.add(url)
                        entries = self._collect_feed_entries(category, url)
                        if entries:
                            all_news.extend(entries)
                            successful_sources += 1
                        if len(all_news) >= self.min_results:
                            break
                    if len(all_news) >= self.min_results:
                        break

        total_sources = len(processed_urls)
        logger.info(
            f"ğŸ“Š æ€»è®¡ä» {successful_sources}/{total_sources} ä¸ªæºè·å– {len(all_news)} æ¡æ–°é—»"
        )
        if len(all_news) < self.min_results:
            logger.warning(
                f"âš ï¸ å½“å‰ä»…è·å– {len(all_news)} æ¡æ–°é—»ï¼Œæœªè¾¾åˆ° {self.min_results} æ¡ç›®æ ‡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è°ƒæ•´ç±»åˆ«é…ç½®ã€‚"
            )
        else:
            logger.success(f"ğŸ¯ å·²è¾¾åˆ° {self.min_results} æ¡ä»¥ä¸Šçš„æ–°é—»æ ·æœ¬é‡è¦æ±‚")
        return all_news

    def clean_and_deduplicate(self, news_list):
        """æ¸…æ´—ä¸å»é‡æ–°é—»"""
        if not news_list:
            logger.warning("No news to clean.")
            return []
        
        # ä½¿ç”¨MD5å“ˆå¸Œå»é‡
        unique_news = {}
        duplicates_removed = 0
        
        for news in news_list:
            title = news.get("title", "").strip()
            link = (news.get("link") or news.get("url") or "").strip().lower()
            source = (news.get("source") or "").strip().lower()
            if not title and not link:
                continue
                
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆåŒæºåŒé“¾æ¥æ‰è§†ä¸ºé‡å¤ï¼‰
            if link:
                dedup_key = link
            else:
                dedup_key = hashlib.md5(f"{title.lower()}::{source}".encode("utf-8")).hexdigest()
            
            if dedup_key not in unique_news:
                unique_news[dedup_key] = news
            else:
                duplicates_removed += 1
        
        cleaned_news = list(unique_news.values())
        logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: åŸå§‹ {len(news_list)} æ¡ -> å»é‡å {len(cleaned_news)} æ¡ (ç§»é™¤ {duplicates_removed} æ¡é‡å¤)")
        return cleaned_news

    def save_news(self, news_list):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if not news_list:
            logger.warning("No news to save.")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_file_path = self.data_dir / "raw" / "raw_news.json"
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å·²ä¿å­˜ {len(news_list)} æ¡æ–°é—»åˆ° {raw_file_path}")

    def safe_request(self, url, retries=3, delay=2):
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(retries):
            try:
                response = requests.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                if response.status_code == 200:
                    return response.text
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt+1} failed for {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(delay)
        
        logger.error(f"All attempts failed for {url}")
        return None

    def create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        logger.info("ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ...")
        
        sample_news = [
            {
                "title": "ç§‘æŠ€è‚¡è¡¨ç°å¼ºåŠ²ï¼ŒAIæŠ€æœ¯æ¨åŠ¨å¸‚åœºåˆ›æ–°",
                "link": "https://example.com/tech-ai-market",
                "published": "2024-10-24T10:00:00Z",
                "summary": "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªè¡Œä¸šçš„åº”ç”¨æ¨åŠ¨äº†ç§‘æŠ€è‚¡çš„å¼ºåŠ²è¡¨ç°ï¼ŒæŠ•èµ„è€…å¯¹AIç›¸å…³å…¬å¸çš„å‰æ™¯ä¿æŒä¹è§‚ã€‚",
                "content": "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šçš„é¢è²Œï¼Œä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°åŒ»ç–—è¯Šæ–­ï¼ŒAIçš„åº”ç”¨èŒƒå›´ä¸æ–­æ‰©å¤§ã€‚æŠ•èµ„è€…å¯¹AIç›¸å…³å…¬å¸çš„å‰æ™¯ä¿æŒä¹è§‚ï¼Œç§‘æŠ€è‚¡å› æ­¤è¡¨ç°å¼ºåŠ²ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "ç§‘æŠ€"
            },
            {
                "title": "å¤®è¡Œè´§å¸æ”¿ç­–è°ƒæ•´ï¼Œé‡‘èå¸‚åœºååº”ç§¯æ",
                "link": "https://example.com/central-bank-policy",
                "published": "2024-10-24T09:30:00Z",
                "summary": "å¤®è¡Œå®£å¸ƒæ–°çš„è´§å¸æ”¿ç­–æªæ–½ï¼Œé‡‘èå¸‚åœºå¯¹æ­¤ååº”ç§¯æï¼Œè‚¡å¸‚å’Œå€ºå¸‚å‡å‡ºç°ä¸Šæ¶¨ã€‚",
                "content": "å¤®è¡Œä»Šæ—¥å®£å¸ƒè°ƒæ•´è´§å¸æ”¿ç­–ï¼ŒåŒ…æ‹¬åˆ©ç‡è°ƒæ•´å’ŒæµåŠ¨æ€§ç®¡ç†æªæ–½ã€‚é‡‘èå¸‚åœºå¯¹æ­¤ååº”ç§¯æï¼Œä¸»è¦è‚¡æŒ‡ä¸Šæ¶¨ï¼Œå€ºåˆ¸æ”¶ç›Šç‡ä¸‹é™ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "é‡‘è"
            },
            {
                "title": "å…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œå›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼º",
                "link": "https://example.com/global-recovery",
                "published": "2024-10-24T08:15:00Z",
                "summary": "æœ€æ–°ç»æµæ•°æ®æ˜¾ç¤ºå…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œå›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼ºï¼Œèµ„é‡‘æµå…¥æ–°å…´å¸‚åœºã€‚",
                "content": "æ ¹æ®æœ€æ–°å‘å¸ƒçš„ç»æµæ•°æ®ï¼Œå…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œåˆ¶é€ ä¸šPMIæŒ‡æ•°å›å‡ï¼Œå°±ä¸šå¸‚åœºæ”¹å–„ã€‚å›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼ºï¼Œèµ„é‡‘å¼€å§‹æµå…¥æ–°å…´å¸‚åœºã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "å›½é™…"
            },
            {
                "title": "è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜",
                "link": "https://example.com/stock-trading",
                "published": "2024-10-24T07:45:00Z",
                "summary": "è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜ï¼ŒæŠ•èµ„è€…å‚ä¸åº¦æ˜æ˜¾æå‡ã€‚",
                "content": "ä»Šæ—¥è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜ã€‚æŠ•èµ„è€…å‚ä¸åº¦æ˜æ˜¾æå‡ï¼Œæœºæ„æŠ•èµ„è€…å’Œä¸ªäººæŠ•èµ„è€…éƒ½åœ¨ç§¯æäº¤æ˜“ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "è‚¡ç¥¨"
            }
        ]
        
        logger.success(f"âœ… å·²åˆ›å»º {len(sample_news)} æ¡ç¤ºä¾‹æ•°æ®")
        return sample_news

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ–°é—»é‡‡é›†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å¤šæºæ–°é—»é‡‡é›†...")
        
        # 1. æŠ“å–æ–°é—»
        raw_news = self.fetch_latest()
        
        # 2. å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œç›´æ¥è¿”å›å¹¶æç¤ºç”¨æˆ·æ£€æŸ¥é…ç½®
        if not raw_news:
            logger.error("âŒ æœªèƒ½ä»RSSæºè·å–æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–RSSæºé…ç½®")
            return []
        
        # 3. æ¸…æ´—å»é‡
        cleaned_news = self.clean_and_deduplicate(raw_news)
        if len(cleaned_news) < self.min_results:
            logger.warning(
                f"âš ï¸ æ¸…æ´—åæ–°é—»æ•°é‡ä¸º {len(cleaned_news)} æ¡ï¼Œæœªè¾¾åˆ° {self.min_results} æ¡ç›®æ ‡ï¼Œå¯å°è¯•æ‰©å±•é‡‡é›†ç±»åˆ«ã€‚"
            )
        
        # 4. ä¿å­˜æ•°æ®
        self.save_news(cleaned_news)
        
        logger.success(f"ğŸ‰ æ–°é—»é‡‡é›†å®Œæˆï¼å…±è·å– {len(cleaned_news)} æ¡æœ‰æ•ˆæ–°é—»")
        return cleaned_news

