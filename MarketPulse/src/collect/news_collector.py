import os
import json
import hashlib
import feedparser
import requests
import time
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger


class NewsCollector:
    """å¤šæºè´¢ç»æ–°é—»é‡‡é›†å™¨ - æ”¯æŒå¤šä¸ªRSSæºï¼Œè‡ªåŠ¨å»é‡å’Œå®¹é”™"""

    def __init__(self, categories=None):
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¶é—´è¿‡æ»¤ï¼šåªæŠ“å–7å¤©å†…çš„æ•°æ®ï¼ˆåœ¨çº¿æ–°é—»é‡‡é›†ï¼‰
        self.cutoff_date = datetime.now() - timedelta(days=7)

        # å¤šæºRSSé…ç½®ï¼ˆæŒ‰ç±»åˆ«ç»„ç»‡ï¼Œä¾¿äºç”¨æˆ·é€‰æ‹©ï¼‰
        # æ›´æ–°ä¸ºå¯ç”¨çš„RSSæºï¼ŒåŒ…å«å›½é™…å’Œä¸­æ–‡æº
        self.category_feeds = {
            "ç§‘æŠ€": [
                "https://feeds.bbci.co.uk/news/technology/rss.xml",
                "https://feeds.reuters.com/reuters/technologyNews",
                "https://feeds.feedburner.com/oreilly/radar"
            ],
            "é‡‘è": [
                "https://feeds.bbci.co.uk/news/business/rss.xml",
                "https://feeds.reuters.com/reuters/businessNews",
                "https://feeds.marketwatch.com/marketwatch/topstories/"
            ],
            "å›½é™…": [
                "https://feeds.bbci.co.uk/news/world/rss.xml",
                "https://feeds.reuters.com/reuters/worldNews",
                "https://feeds.a.dj.com/rss/RSSWorldNews.xml",
                "https://feeds.cnn.com/rss/edition.rss",
                "https://feeds.npr.org/1001/rss.xml",
                "https://feeds.nytimes.com/nyt/World.xml"
            ],
            "è‚¡ç¥¨": [
                "https://feeds.marketwatch.com/marketwatch/marketpulse/",
                "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
                "https://feeds.finance.yahoo.com/rss/2.0/headline"
            ]
        }

        self.category_alias_map = {
            "tech": "ç§‘æŠ€",
            "ç§‘æŠ€": "ç§‘æŠ€",
            "technology": "ç§‘æŠ€",
            "finance": "é‡‘è",
            "é‡‘è": "é‡‘è",
            "international": "å›½é™…",
            "å›½é™…": "å›½é™…",
            "global": "å›½é™…",
            "stock": "è‚¡ç¥¨",
            "stocks": "è‚¡ç¥¨",
            "è‚¡ç¥¨": "è‚¡ç¥¨"
        }

        self.selected_categories = self._normalize_categories(categories)

    def set_categories(self, categories=None):
        """æ›´æ–°ç”¨æˆ·é€‰æ‹©çš„ç±»åˆ«"""
        self.selected_categories = self._normalize_categories(categories)

    def _normalize_categories(self, categories=None):
        if not categories:
            return list(self.category_feeds.keys())

        normalized = []
        for category in categories:
            if not category:
                continue
            key = self.category_alias_map.get(str(category).strip().lower(), category)
            if key in self.category_feeds and key not in normalized:
                normalized.append(key)
        return normalized or list(self.category_feeds.keys())
    
    def _is_recent_news(self, published_str: str) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨3å¤©å†…å‘å¸ƒ"""
        if not published_str:
            return True  # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«
        
        try:
            # å°è¯•è§£æå„ç§æ—¶é—´æ ¼å¼
            published_time = None
            
            # å¸¸è§çš„æ—¶é—´æ ¼å¼
            time_formats = [
                "%a, %d %b %Y %H:%M:%S %Z",  # RFC 2822
                "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822 with timezone
                "%Y-%m-%d %H:%M:%S",          # ISO format
                "%Y-%m-%dT%H:%M:%S",         # ISO format with T
                "%Y-%m-%dT%H:%M:%SZ",       # ISO format with Z
                "%Y-%m-%dT%H:%M:%S.%fZ",    # ISO format with microseconds
                "%a, %d %b %Y %H:%M:%S",    # Without timezone
            ]
            
            for fmt in time_formats:
                try:
                    published_time = datetime.strptime(published_str, fmt)
                    break
                except ValueError:
                    continue
            
            if published_time is None:
                # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨feedparserçš„æ—¶é—´è§£æ
                try:
                    import email.utils
                    published_time = email.utils.parsedate_to_datetime(published_str)
                except:
                    return True  # è§£æå¤±è´¥æ—¶é»˜è®¤åŒ…å«
            
            # æ£€æŸ¥æ˜¯å¦åœ¨3å¤©å†…
            return published_time >= self.cutoff_date
            
        except Exception as e:
            logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {published_str} - {e}")
            return True  # è§£æå¤±è´¥æ—¶é»˜è®¤åŒ…å«

    def fetch_latest(self):
        """ä»å¤šä¸ªRSSæºæŠ“å–æ–°é—»"""
        all_news = []
        successful_sources = 0

        categories = self.selected_categories or list(self.category_feeds.keys())
        planned_feeds = []
        for category in categories:
            feeds = self.category_feeds.get(category, [])
            planned_feeds.extend([(category, url) for url in feeds])

        if not planned_feeds:
            logger.warning("æœªæ‰¾åˆ°åŒ¹é…çš„RSSæºï¼Œä½¿ç”¨æ‰€æœ‰é»˜è®¤æºã€‚")
            for category, urls in self.category_feeds.items():
                planned_feeds.extend([(category, url) for url in urls])

        for category, url in planned_feeds:
            logger.info(f"Fetching {category} news from {url} ...")
            try:
                # ä½¿ç”¨feedparseræŠ“å–RSSï¼Œå¢åŠ è¶…æ—¶å’Œé‡è¯•æœºåˆ¶
                feed = feedparser.parse(url)

                # æ£€æŸ¥RSSè§£æçŠ¶æ€
                if hasattr(feed, 'bozo') and feed.bozo:
                    logger.warning(f"RSSè§£æè­¦å‘Š: {url} - {getattr(feed, 'bozo_exception', 'Unknown error')}")

                if not feed.entries:
                    logger.warning(f"No entries found in {url}")
                    continue

                source_news = []
                for entry in feed.entries:
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨3å¤©å†…
                    if not self._is_recent_news(entry.get("published", "")):
                        continue
                    
                    # æ ‡å‡†åŒ–æ•°æ®ç»“æ„ï¼Œå¢åŠ æ›´å¤šå­—æ®µå¤„ç†
                    item = {
                        "title": entry.get("title", "").strip(),
                        "link": entry.get("link", "").strip(),
                        "published": entry.get("published", ""),
                        "summary": entry.get("summary", "").strip(),
                        "content": entry.get("content", [{}])[0].get("value", "") if entry.get("content") else "",
                        "source": url,  # æ·»åŠ æ¥æºæ ‡è¯†
                        "category": category
                    }

                    # è¿‡æ»¤ç©ºæ ‡é¢˜å’Œæ— æ•ˆé“¾æ¥
                    if item["title"] and item["link"]:
                        source_news.append(item)

                if source_news:
                    all_news.extend(source_news)
                    successful_sources += 1
                    logger.success(f"âœ… æˆåŠŸä» {url} è·å– {len(source_news)} æ¡ {category} ç±»æ–°é—»")
                else:
                    logger.warning(f"ä» {url} è·å–çš„æ–°é—»ä¸ºç©ºæˆ–æ— æ•ˆ")

            except Exception as e:
                logger.error(f"âŒ ä» {url} æŠ“å–å¤±è´¥: {e}")
                continue

        logger.info(
            f"ğŸ“Š æ€»è®¡ä» {successful_sources}/{len(planned_feeds)} ä¸ªæºè·å– {len(all_news)} æ¡æ–°é—»"
        )
        return all_news

    def clean_and_deduplicate(self, news_list):
        """æ¸…æ´—ä¸å»é‡æ–°é—»"""
        if not news_list:
            logger.warning("No news to clean.")
            return []
        
        # ä½¿ç”¨MD5å“ˆå¸Œå»é‡
        unique_news = {}
        duplicates_removed = 0
        
        for news in news_list:
            title = news.get("title", "").strip()
            if not title:
                continue
                
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            content_hash = hashlib.md5(title.encode("utf-8")).hexdigest()
            
            if content_hash not in unique_news:
                unique_news[content_hash] = news
            else:
                duplicates_removed += 1
        
        cleaned_news = list(unique_news.values())
        logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: åŸå§‹ {len(news_list)} æ¡ -> å»é‡å {len(cleaned_news)} æ¡ (ç§»é™¤ {duplicates_removed} æ¡é‡å¤)")
        return cleaned_news

    def save_news(self, news_list):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if not news_list:
            logger.warning("No news to save.")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_file_path = self.data_dir / "raw" / "raw_news.json"
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å·²ä¿å­˜ {len(news_list)} æ¡æ–°é—»åˆ° {raw_file_path}")

    def safe_request(self, url, retries=3, delay=2):
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(retries):
            try:
                response = requests.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                if response.status_code == 200:
                    return response.text
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt+1} failed for {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(delay)
        
        logger.error(f"All attempts failed for {url}")
        return None

    def create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        logger.info("ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ...")
        
        sample_news = [
            {
                "title": "ç§‘æŠ€è‚¡è¡¨ç°å¼ºåŠ²ï¼ŒAIæŠ€æœ¯æ¨åŠ¨å¸‚åœºåˆ›æ–°",
                "link": "https://example.com/tech-ai-market",
                "published": "2024-10-24T10:00:00Z",
                "summary": "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªè¡Œä¸šçš„åº”ç”¨æ¨åŠ¨äº†ç§‘æŠ€è‚¡çš„å¼ºåŠ²è¡¨ç°ï¼ŒæŠ•èµ„è€…å¯¹AIç›¸å…³å…¬å¸çš„å‰æ™¯ä¿æŒä¹è§‚ã€‚",
                "content": "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šçš„é¢è²Œï¼Œä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°åŒ»ç–—è¯Šæ–­ï¼ŒAIçš„åº”ç”¨èŒƒå›´ä¸æ–­æ‰©å¤§ã€‚æŠ•èµ„è€…å¯¹AIç›¸å…³å…¬å¸çš„å‰æ™¯ä¿æŒä¹è§‚ï¼Œç§‘æŠ€è‚¡å› æ­¤è¡¨ç°å¼ºåŠ²ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "ç§‘æŠ€"
            },
            {
                "title": "å¤®è¡Œè´§å¸æ”¿ç­–è°ƒæ•´ï¼Œé‡‘èå¸‚åœºååº”ç§¯æ",
                "link": "https://example.com/central-bank-policy",
                "published": "2024-10-24T09:30:00Z",
                "summary": "å¤®è¡Œå®£å¸ƒæ–°çš„è´§å¸æ”¿ç­–æªæ–½ï¼Œé‡‘èå¸‚åœºå¯¹æ­¤ååº”ç§¯æï¼Œè‚¡å¸‚å’Œå€ºå¸‚å‡å‡ºç°ä¸Šæ¶¨ã€‚",
                "content": "å¤®è¡Œä»Šæ—¥å®£å¸ƒè°ƒæ•´è´§å¸æ”¿ç­–ï¼ŒåŒ…æ‹¬åˆ©ç‡è°ƒæ•´å’ŒæµåŠ¨æ€§ç®¡ç†æªæ–½ã€‚é‡‘èå¸‚åœºå¯¹æ­¤ååº”ç§¯æï¼Œä¸»è¦è‚¡æŒ‡ä¸Šæ¶¨ï¼Œå€ºåˆ¸æ”¶ç›Šç‡ä¸‹é™ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "é‡‘è"
            },
            {
                "title": "å…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œå›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼º",
                "link": "https://example.com/global-recovery",
                "published": "2024-10-24T08:15:00Z",
                "summary": "æœ€æ–°ç»æµæ•°æ®æ˜¾ç¤ºå…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œå›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼ºï¼Œèµ„é‡‘æµå…¥æ–°å…´å¸‚åœºã€‚",
                "content": "æ ¹æ®æœ€æ–°å‘å¸ƒçš„ç»æµæ•°æ®ï¼Œå…¨çƒç»æµå¤è‹è¿¹è±¡æ˜æ˜¾ï¼Œåˆ¶é€ ä¸šPMIæŒ‡æ•°å›å‡ï¼Œå°±ä¸šå¸‚åœºæ”¹å–„ã€‚å›½é™…æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼ºï¼Œèµ„é‡‘å¼€å§‹æµå…¥æ–°å…´å¸‚åœºã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "å›½é™…"
            },
            {
                "title": "è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜",
                "link": "https://example.com/stock-trading",
                "published": "2024-10-24T07:45:00Z",
                "summary": "è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜ï¼ŒæŠ•èµ„è€…å‚ä¸åº¦æ˜æ˜¾æå‡ã€‚",
                "content": "ä»Šæ—¥è‚¡ç¥¨å¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œæˆäº¤é‡åˆ›è¿‘æœŸæ–°é«˜ã€‚æŠ•èµ„è€…å‚ä¸åº¦æ˜æ˜¾æå‡ï¼Œæœºæ„æŠ•èµ„è€…å’Œä¸ªäººæŠ•èµ„è€…éƒ½åœ¨ç§¯æäº¤æ˜“ã€‚",
                "source": "ç¤ºä¾‹æ•°æ®æº",
                "category": "è‚¡ç¥¨"
            }
        ]
        
        logger.success(f"âœ… å·²åˆ›å»º {len(sample_news)} æ¡ç¤ºä¾‹æ•°æ®")
        return sample_news

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ–°é—»é‡‡é›†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å¤šæºæ–°é—»é‡‡é›†...")
        
        # 1. æŠ“å–æ–°é—»
        raw_news = self.fetch_latest()
        
        # 2. å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
        if not raw_news:
            logger.warning("âš ï¸ æœªèƒ½ä»RSSæºè·å–æ–°é—»ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º")
            raw_news = self.create_sample_data()
        
        # 3. æ¸…æ´—å»é‡
        cleaned_news = self.clean_and_deduplicate(raw_news)
        
        # 4. ä¿å­˜æ•°æ®
        self.save_news(cleaned_news)
        
        logger.success(f"ğŸ‰ æ–°é—»é‡‡é›†å®Œæˆï¼å…±è·å– {len(cleaned_news)} æ¡æœ‰æ•ˆæ–°é—»")
        return cleaned_news

