import os
import json
import hashlib
import feedparser
import requests
import time
from datetime import datetime
from pathlib import Path
from loguru import logger


class NewsCollector:
    """å¤šæºè´¢ç»æ–°é—»é‡‡é›†å™¨ - æ”¯æŒå¤šä¸ªRSSæºï¼Œè‡ªåŠ¨å»é‡å’Œå®¹é”™"""

    def __init__(self, categories=None):
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)

        # å¤šæºRSSé…ç½®ï¼ˆæŒ‰ç±»åˆ«ç»„ç»‡ï¼Œä¾¿äºç”¨æˆ·é€‰æ‹©ï¼‰
        self.category_feeds = {
            "ç§‘æŠ€": [
                "https://rss.sina.com.cn/tech/it/itroll.xml",
                "https://rss.sina.com.cn/tech/tele/tele_it.xml",
                "https://www.thepaper.cn/channel_27262?page=1&RSS=1"
            ],
            "é‡‘è": [
                "https://rss.sina.com.cn/finance/china/focus15.xml",
                "https://finance.eastmoney.com/rss/chaoguxinwen.xml",
                "https://www.cs.com.cn/rss/finance.xml"
            ],
            "å›½é™…": [
                "https://rss.sina.com.cn/finance/global/index.xml",
                "https://www.ftchinese.com/rss/news",
                "https://www.reuters.com/world/china/rss"
            ],
            "è‚¡ç¥¨": [
                "https://rss.sina.com.cn/finance/china/stock20.xml",
                "https://finance.eastmoney.com/rss/stock.xml",
                "https://www.21jingji.com/rss/stock.xml"
            ]
        }

        self.category_alias_map = {
            "tech": "ç§‘æŠ€",
            "ç§‘æŠ€": "ç§‘æŠ€",
            "technology": "ç§‘æŠ€",
            "finance": "é‡‘è",
            "é‡‘è": "é‡‘è",
            "international": "å›½é™…",
            "å›½é™…": "å›½é™…",
            "global": "å›½é™…",
            "stock": "è‚¡ç¥¨",
            "stocks": "è‚¡ç¥¨",
            "è‚¡ç¥¨": "è‚¡ç¥¨"
        }

        self.selected_categories = self._normalize_categories(categories)

    def set_categories(self, categories=None):
        """æ›´æ–°ç”¨æˆ·é€‰æ‹©çš„ç±»åˆ«"""
        self.selected_categories = self._normalize_categories(categories)

    def _normalize_categories(self, categories=None):
        if not categories:
            return list(self.category_feeds.keys())

        normalized = []
        for category in categories:
            if not category:
                continue
            key = self.category_alias_map.get(str(category).strip().lower(), category)
            if key in self.category_feeds and key not in normalized:
                normalized.append(key)
        return normalized or list(self.category_feeds.keys())

    def fetch_latest(self):
        """ä»å¤šä¸ªRSSæºæŠ“å–æ–°é—»"""
        all_news = []
        successful_sources = 0

        categories = self.selected_categories or list(self.category_feeds.keys())
        planned_feeds = []
        for category in categories:
            feeds = self.category_feeds.get(category, [])
            planned_feeds.extend([(category, url) for url in feeds])

        if not planned_feeds:
            logger.warning("æœªæ‰¾åˆ°åŒ¹é…çš„RSSæºï¼Œä½¿ç”¨æ‰€æœ‰é»˜è®¤æºã€‚")
            for category, urls in self.category_feeds.items():
                planned_feeds.extend([(category, url) for url in urls])

        for category, url in planned_feeds:
            logger.info(f"Fetching {category} news from {url} ...")
            try:
                # ä½¿ç”¨feedparseræŠ“å–RSS
                feed = feedparser.parse(url)

                if not feed.entries:
                    logger.warning(f"No entries found in {url}")
                    continue

                source_news = []
                for entry in feed.entries:
                    # æ ‡å‡†åŒ–æ•°æ®ç»“æ„
                    item = {
                        "title": entry.get("title", "").strip(),
                        "link": entry.get("link", "").strip(),
                        "published": entry.get("published", ""),
                        "summary": entry.get("summary", "").strip(),
                        "source": url,  # æ·»åŠ æ¥æºæ ‡è¯†
                        "category": category
                    }

                    # è¿‡æ»¤ç©ºæ ‡é¢˜
                    if item["title"]:
                        source_news.append(item)

                all_news.extend(source_news)
                successful_sources += 1
                logger.success(f"âœ… æˆåŠŸä» {url} è·å– {len(source_news)} æ¡ {category} ç±»æ–°é—»")

            except Exception as e:
                logger.error(f"âŒ ä» {url} æŠ“å–å¤±è´¥: {e}")
                continue

        logger.info(
            f"ğŸ“Š æ€»è®¡ä» {successful_sources}/{len(planned_feeds)} ä¸ªæºè·å– {len(all_news)} æ¡æ–°é—»"
        )
        return all_news

    def clean_and_deduplicate(self, news_list):
        """æ¸…æ´—ä¸å»é‡æ–°é—»"""
        if not news_list:
            logger.warning("No news to clean.")
            return []
        
        # ä½¿ç”¨MD5å“ˆå¸Œå»é‡
        unique_news = {}
        duplicates_removed = 0
        
        for news in news_list:
            title = news.get("title", "").strip()
            if not title:
                continue
                
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            content_hash = hashlib.md5(title.encode("utf-8")).hexdigest()
            
            if content_hash not in unique_news:
                unique_news[content_hash] = news
            else:
                duplicates_removed += 1
        
        cleaned_news = list(unique_news.values())
        logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: åŸå§‹ {len(news_list)} æ¡ -> å»é‡å {len(cleaned_news)} æ¡ (ç§»é™¤ {duplicates_removed} æ¡é‡å¤)")
        return cleaned_news

    def save_news(self, news_list):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if not news_list:
            logger.warning("No news to save.")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_file_path = self.data_dir / "raw" / "raw_news.json"
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å·²ä¿å­˜ {len(news_list)} æ¡æ–°é—»åˆ° {raw_file_path}")

    def safe_request(self, url, retries=3, delay=2):
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(retries):
            try:
                response = requests.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                if response.status_code == 200:
                    return response.text
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt+1} failed for {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(delay)
        
        logger.error(f"All attempts failed for {url}")
        return None

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ–°é—»é‡‡é›†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å¤šæºæ–°é—»é‡‡é›†...")
        
        # 1. æŠ“å–æ–°é—»
        raw_news = self.fetch_latest()
        
        if not raw_news:
            logger.error("âŒ æœªèƒ½è·å–ä»»ä½•æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–RSSæº")
            return []
        
        # 2. æ¸…æ´—å»é‡
        cleaned_news = self.clean_and_deduplicate(raw_news)
        
        # 3. ä¿å­˜æ•°æ®
        self.save_news(cleaned_news)
        
        logger.success(f"ğŸ‰ æ–°é—»é‡‡é›†å®Œæˆï¼å…±è·å– {len(cleaned_news)} æ¡æœ‰æ•ˆæ–°é—»")
        return cleaned_news

