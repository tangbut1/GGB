import os
import json
import hashlib
import feedparser
import requests
import time
from datetime import datetime
from pathlib import Path
from loguru import logger


class NewsCollector:
    """å¤šæºè´¢ç»æ–°é—»é‡‡é›†å™¨ - æ”¯æŒå¤šä¸ªRSSæºï¼Œè‡ªåŠ¨å»é‡å’Œå®¹é”™"""

    def __init__(self):
        self.data_dir = Path(__file__).resolve().parents[2] / "data"
        self.data_dir.mkdir(exist_ok=True)
        
        # å¤šæºRSSé…ç½®ï¼ˆè‡ªåŠ¨fallbackï¼‰
        self.rss_feeds = [
            "https://rss.sina.com.cn/finance/china/focus15.xml",   # æ–°æµªè´¢ç»ç„¦ç‚¹
            "https://rss.sina.com.cn/finance/global/index.xml",   # æ–°æµªå›½é™…è´¢ç»
            "https://rss.sina.com.cn/finance/china/stock20.xml",  # æ–°æµªè‚¡ç¥¨
            "https://finance.eastmoney.com/rss/stock.xml",        # ä¸œæ–¹è´¢å¯Œ
            "https://finance.yahoo.com/news/rssindex",             # é›…è™è´¢ç»
            "https://feeds.finance.yahoo.com/rss/2.0/headline",    # é›…è™è´¢ç»å¤´æ¡
        ]

    def fetch_latest(self):
        """ä»å¤šä¸ªRSSæºæŠ“å–æ–°é—»"""
        all_news = []
        successful_sources = 0
        
        for url in self.rss_feeds:
            logger.info(f"Fetching news from {url} ...")
            try:
                # ä½¿ç”¨feedparseræŠ“å–RSS
                feed = feedparser.parse(url)
                
                if not feed.entries:
                    logger.warning(f"No entries found in {url}")
                    continue
                
                source_news = []
                for entry in feed.entries:
                    # æ ‡å‡†åŒ–æ•°æ®ç»“æ„
                    item = {
                        "title": entry.get("title", "").strip(),
                        "link": entry.get("link", "").strip(),
                        "published": entry.get("published", ""),
                        "summary": entry.get("summary", "").strip(),
                        "source": url  # æ·»åŠ æ¥æºæ ‡è¯†
                    }
                    
                    # è¿‡æ»¤ç©ºæ ‡é¢˜
                    if item["title"]:
                        source_news.append(item)
                
                all_news.extend(source_news)
                successful_sources += 1
                logger.success(f"âœ… æˆåŠŸä» {url} è·å– {len(source_news)} æ¡æ–°é—»")
                
            except Exception as e:
                logger.error(f"âŒ ä» {url} æŠ“å–å¤±è´¥: {e}")
                continue
        
        logger.info(f"ğŸ“Š æ€»è®¡ä» {successful_sources}/{len(self.rss_feeds)} ä¸ªæºè·å– {len(all_news)} æ¡æ–°é—»")
        return all_news

    def clean_and_deduplicate(self, news_list):
        """æ¸…æ´—ä¸å»é‡æ–°é—»"""
        if not news_list:
            logger.warning("No news to clean.")
            return []
        
        # ä½¿ç”¨MD5å“ˆå¸Œå»é‡
        unique_news = {}
        duplicates_removed = 0
        
        for news in news_list:
            title = news.get("title", "").strip()
            if not title:
                continue
                
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            content_hash = hashlib.md5(title.encode("utf-8")).hexdigest()
            
            if content_hash not in unique_news:
                unique_news[content_hash] = news
            else:
                duplicates_removed += 1
        
        cleaned_news = list(unique_news.values())
        logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: åŸå§‹ {len(news_list)} æ¡ -> å»é‡å {len(cleaned_news)} æ¡ (ç§»é™¤ {duplicates_removed} æ¡é‡å¤)")
        return cleaned_news

    def save_news(self, news_list):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if not news_list:
            logger.warning("No news to save.")
            return
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_file_path = self.data_dir / "raw" / "raw_news.json"
        raw_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(raw_file_path, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å·²ä¿å­˜ {len(news_list)} æ¡æ–°é—»åˆ° {raw_file_path}")

    def safe_request(self, url, retries=3, delay=2):
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(retries):
            try:
                response = requests.get(url, timeout=10, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                if response.status_code == 200:
                    return response.text
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt+1} failed for {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(delay)
        
        logger.error(f"All attempts failed for {url}")
        return None

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ–°é—»é‡‡é›†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å¤šæºæ–°é—»é‡‡é›†...")
        
        # 1. æŠ“å–æ–°é—»
        raw_news = self.fetch_latest()
        
        if not raw_news:
            logger.error("âŒ æœªèƒ½è·å–ä»»ä½•æ–°é—»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–RSSæº")
            return []
        
        # 2. æ¸…æ´—å»é‡
        cleaned_news = self.clean_and_deduplicate(raw_news)
        
        # 3. ä¿å­˜æ•°æ®
        self.save_news(cleaned_news)
        
        logger.success(f"ğŸ‰ æ–°é—»é‡‡é›†å®Œæˆï¼å…±è·å– {len(cleaned_news)} æ¡æœ‰æ•ˆæ–°é—»")
        return cleaned_news

